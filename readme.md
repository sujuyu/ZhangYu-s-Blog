### 欢迎来到 ZhangYu 的blog

现在更新了两篇文章, 希望后面还能继续更新, 欢迎一起交流

#### 使用Intel onednn后端量化Pytorch模型 加速PyTorch模型在Intel CPU上的推理

- [Accelerating PyTorch Model Inference on Intel CPUs using the Intel oneDNN Backend for Quantization](https://github.com/sujuyu/ZhangYu-s-Blog/tree/main/torch_onednn_quantization)



#### 基于图追踪和预编译技术的Pytorch 2+模型部署链路初探
- 利用PyTorch Dynamo的export功能与AOT编译技术，对nn.Module进行深度解析与优化，预先编译成高性能的动态库，进而实现在C++环境中调用，以脱离Python运行时环境的依赖的入门教程
- [A beginner's guide to leveraging PyTorch Dynamo's export feature and AOT compilation to deeply analyze and optimize nn.Module, pre-compiling it into a high-performance dynamic library for direct C++ usage, thereby eliminating Python runtime dependence.](https://github.com/sujuyu/ZhangYu-s-Blog/tree/main/torch_fx_aot_chapter)